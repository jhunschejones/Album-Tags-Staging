# Album-Tags-Staging
This is a staging code-base for albumtags.com that is used to test larger site changes before pushing to the main repository. The main albumtags.com repository can be found [here](https://github.com/jhunschejones/album-tags).

### Updates 10/11/18
1. In this update I added some security changes. These include data validation to the `POST` API endpoint to prevent cross-site scripting and functionality on the client to parse data before adding it to the DOM. The approach I took was replacing special characters with their HTML equivalents. This prevents malicious content from affecting the page doesn't affect how the content looks if the characters were intended. I also added functionality to the `POST` Ajax to check for scripting-attempts.
2. This is the first update with some focus on improving security and efficiency in the client side scripts. There is more work to be done here, but I've removed extra console logs and added a counter to the `POST` functionality so that if a new tag doesn't load after being added the API is called a limited amount of times before the page is automatically refreshed *(previously there was no limit!)* This still addresses the original goal of providing a seamless experience on slow networks while simultaneously being judicious about how many API calls the front end can make and finally solving for when a page refresh is required to load the new tags due to an *extra* slow network.

### Updates 10/07/18
1. In this update the `POST` endpoint was removed and instead `{ upsert: true }` was added to the PUT endpoint so that if a record does not exist for an album it will create a new one. The client-side update script has also been updated so that it does not POST place-holder records when the page is accessed. This not only simplifies the code, it helps users on a slower connection because the update page does not have to wait for the `POST` request to finish before adding a new record.
2. Worker count was decreased yesterday as 8 workers was using more memory than desired. This commit moves the count up from 4 to 5 workers.
3. The app will now send a custom event and metric to New Relic when a worker dies, triggering an alert. There will need to be more work around making sure relevant data about the cause of a worker crash is available but this is the first step in gaining visibility into the cluster process. Though it is an advantage to be able to spin up a new worker when the app crashes without the user being aware, it does mean a larger problem could exist without the developers knowing.

### Updates 10/06/18
1. All the routes for the app have been refactored. Database and external calls have been pulled out of the routes for each page and placed in the `api.v1.route.js` route. All client JavaScript files have been updated with the new endpoints. This allows for removal of some duplicate code and easier updates for future backend changes.
2. The `require(‘newrelic’)` statement has been moved up from the app.js file into the `www` file so that it is the first package when Heroku calls `npm start`.
3. The `www` file was updated with Cluster functionality. The app should now serve up one version of the backend per CPU available. This should help handle multiple site requests better and make better use of the multi-core resources available on the Heroku dyno. There is a chance it will affect how data reports to New Relic, as well as database calls, so we are only deploying this in the staging code-base to start with.
